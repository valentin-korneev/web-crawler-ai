import asyncio
import re
import aiohttp
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

from app.models.contractor import Contractor
from app.models.webpage import WebPage
from app.models.forbidden_word import ForbiddenWord
from app.models.scan_session import ScanSession
from app.models.scan_result import Violation
from app.services.queue_service import queue_service
from app.core.logging import logger


class ScannerService:
    def __init__(self):
        self.session: aiohttp.ClientSession | None = None
        
    async def start_session(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ HTTP —Å–µ—Å—Å–∏–∏"""
        if not self.session:
            connector = aiohttp.TCPConnector(limit=10, limit_per_host=5)
            self.session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=30),
                connector=connector,
                headers={
                    'User-Agent': 'HuginnBot/1.0 (+https://huginn.local)'
                }
            )
    
    async def close_session(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ HTTP —Å–µ—Å—Å–∏–∏"""
        if self.session:
            await self.session.close()
            self.session = None
    
    async def scan_contractor(self, contractor_id: int, start_url: str | None = None, session_id: int = None):
        """–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç–∞ - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏ –≤ –æ—á–µ—Ä–µ–¥—å"""
        await self.start_session()
        
        try:
            contractor = await Contractor.get(id=contractor_id)
            if not contractor.is_active:
                await logger.info(f"‚è∏Ô∏è Contractor {contractor_id} is not active, skipping scan")
                return
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π URL
            if not start_url:
                start_url = f"https://{contractor.domain}"
            
            await logger.info(f"üîç Scanning contractor {contractor_id} ({contractor.name}) - URL: {start_url}, session_id: {session_id}")
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–µ—Å—Å–∏—é —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
            scan_session = None
            if session_id:
                scan_session = await ScanSession.get_or_none(id=session_id)
                if not scan_session:
                    await logger.error(f"‚ùå Scan session {session_id} not found")
                    return
                await logger.info(f"üìù Using existing scan session {scan_session.id}")
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ scan_session –∑–∞–≥—Ä—É–∂–µ–Ω –ø—Ä–∞–≤–∏–ª—å–Ω–æ
                await logger.info(f"üîç Scan session type: {type(scan_session)}, id: {scan_session.id}")
            else:
                await logger.warning(f"‚ö†Ô∏è No session_id provided, scanning without session tracking")
            
            # –ü–æ–ª—É—á–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
            forbidden_words = await ForbiddenWord.filter(is_active=True)
            forbidden_words_data = [
                {
                    'word': word.word,
                    'use_regex': word.use_regex,
                    'case_sensitive': word.case_sensitive,
                    'severity': word.severity
                }
                for word in forbidden_words
            ]
            await logger.info(f"üìù Found {len(forbidden_words_data)} forbidden words for scanning")
            
            # –û–±—ã—á–Ω–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ - –ø—Ä–æ–≤–µ—Ä—è–µ–º TTL
            # –ï—Å–ª–∏ –µ—Å—Ç—å session_id, –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –≤ —Ä–∞–º–∫–∞—Ö —ç—Ç–æ–π —Å–µ—Å—Å–∏–∏
            if scan_session:
                existing_page = await WebPage.filter(
                    contractor=contractor,
                    url=start_url,
                    scan_session=scan_session
                ).first()
                
                if existing_page:
                    await logger.info(f"‚è≠Ô∏è Page {start_url} already scanned in session {scan_session.id}, skipping")
                    return
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç session_id, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—É—é –ª–æ–≥–∏–∫—É TTL
                existing_page = await WebPage.filter(
                    contractor=contractor,
                    url=start_url,
                    last_scanned__gte=datetime.utcnow() - timedelta(hours=1)
                ).first()
                
                if existing_page:
                    await logger.info(f"‚è≠Ô∏è Page {start_url} was recently scanned, skipping")
                    return
            
            # –°–∫–∞–Ω–∏—Ä—É–µ–º –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É
            await self._scan_single_page(
                contractor=contractor,
                url=start_url,
                forbidden_words=forbidden_words_data,
                scan_session=scan_session,
                max_pages=contractor.max_pages or 100
            )
            
            # –ó–∞–≤–µ—Ä—à–∞–µ–º —Å–µ—Å—Å–∏—é —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
            if scan_session:
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–µ—Å—Å–∏–∏
                if scan_session:
                    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ —Å–µ—Å—Å–∏–∏
                    pages_in_session = await WebPage.filter(scan_session=scan_session).count()
                    
                    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞—Ä—É—à–µ–Ω–∏–π –≤ —Å–µ—Å—Å–∏–∏
                    violations_in_session = await Violation.filter(webpage__scan_session=scan_session).count()
                    
                    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü —Å –Ω–∞—Ä—É—à–µ–Ω–∏—è–º–∏ –≤ —Å–µ—Å—Å–∏–∏
                    pages_with_violations = await WebPage.filter(
                        scan_session=scan_session,
                        violations_found=True
                    ).count()
                    
                    scan_session.pages_scanned = pages_in_session
                    scan_session.pages_with_violations = pages_with_violations
                    scan_session.total_violations = violations_in_session
                    scan_session.status = 'completed'
                    scan_session.completed_at = datetime.utcnow()
                    await scan_session.save()
                    
                    await logger.info(f"üìä Session {scan_session.id} completed: {pages_in_session} pages, {violations_in_session} violations")
                
                await logger.info(f"‚úÖ Scan completed for contractor {contractor.name}")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç–∞
            contractor.last_check = datetime.utcnow()
            contractor.next_check = datetime.utcnow() + timedelta(hours=contractor.get_scan_interval_hours())
            await contractor.save()
            
            await logger.info(f"‚úÖ Completed scanning page {start_url} for contractor {contractor_id}")
            
        except Exception as e:
            await logger.error(f"‚ùå Error scanning contractor {contractor_id}: {e}")
            await logger.exception("Full traceback:")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å —Å–µ—Å—Å–∏–∏ –Ω–∞ failed
            if scan_session:
                scan_session.status = 'failed'
                scan_session.completed_at = datetime.utcnow()
                scan_session.error_message = str(e)
                await scan_session.save()
                await logger.info(f"‚ùå Marked scan session {scan_session.id} as failed")
            
            raise
        finally:
            # –ù–ï –∑–∞–∫—Ä—ã–≤–∞–µ–º —Å–µ—Å—Å–∏—é - –æ–Ω–∞ –¥–æ–ª–∂–Ω–∞ –∂–∏—Ç—å –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            pass
    
    async def _scan_single_page(
        self,
        contractor: Contractor,
        url: str,
        forbidden_words: List[Dict[str, Any]],
        max_pages: int,
        scan_session: ScanSession = None
    ):
        """–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            await logger.info(f"üìÑ Fetching page: {url}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º TTL –¥–ª—è —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            # –ï—Å–ª–∏ –µ—Å—Ç—å session_id, –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –≤ —Ä–∞–º–∫–∞—Ö —ç—Ç–æ–π —Å–µ—Å—Å–∏–∏
            if scan_session:
                existing_page = await WebPage.filter(
                    contractor=contractor,
                    url=url,
                    scan_session=scan_session
                ).first()
                
                if existing_page:
                    await logger.info(f"‚è≠Ô∏è Page {url} already scanned in session {scan_session.id}, skipping")
                    return
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç session_id, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—É—é –ª–æ–≥–∏–∫—É TTL
                existing_page = await WebPage.filter(
                    contractor=contractor,
                    url=url,
                    last_scanned__gte=datetime.utcnow() - timedelta(hours=1)
                ).first()
                
                if existing_page:
                    await logger.info(f"‚è≠Ô∏è Page {url} was recently scanned, skipping")
                    return
            
            # –°–∫–∞–Ω–∏—Ä—É–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            page_data = await self._fetch_page(url)
            if not page_data:
                await logger.warning(f"‚ö†Ô∏è Failed to fetch page: {url}")
                return
            
            await logger.info(f"üìä Page fetched successfully: {url} (HTTP {page_data.get('http_status')}, {page_data.get('response_time', 0):.2f}s)")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            webpage = await self._save_webpage(contractor, url, page_data, scan_session)
            await logger.info(f"üíæ Page saved to database: {url}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–∞—Ä—É—à–µ–Ω–∏—è
            violations = await self._check_violations(page_data, forbidden_words)
            if violations:
                await logger.warning(f"üö® Found {len(violations)} violations on page: {url}")
                await self._save_violations(webpage, violations)
                await queue_service.publish_violation_notification({
                    "contractor_id": contractor.id,
                    "contractor_name": contractor.name,
                    "url": url,
                    "violations": violations
                })
            else:
                await logger.info(f"‚úÖ No violations found on page: {url}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏ –¥–æ–±–∞–≤–ª—è–µ–º –∏—Ö –≤ –æ—á–µ—Ä–µ–¥—å
            links = await self._extract_links(page_data['html'], contractor.domain)
            await logger.info(f"üîó Extracted {len(links)} links from page: {url}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç–∞
            total_pages = await WebPage.filter(contractor=contractor).count()
            
            added_to_queue = 0
            for link in links:
                if total_pages + added_to_queue >= max_pages:
                    await logger.info(f"‚èπÔ∏è Reached max pages limit ({max_pages}) for contractor {contractor.id}")
                    break
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ —Å—Ç—Ä–∞–Ω–∏—Ü–∞
                # –ï—Å–ª–∏ –µ—Å—Ç—å session_id, –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –≤ —Ä–∞–º–∫–∞—Ö —ç—Ç–æ–π —Å–µ—Å—Å–∏–∏
                if scan_session:
                    existing_page = await WebPage.filter(contractor=contractor, url=link, scan_session=scan_session).first()
                else:
                    existing_page = await WebPage.filter(contractor=contractor, url=link).first()
                
                if not existing_page:
                    # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á—É –≤ –æ—á–µ—Ä–µ–¥—å
                    await queue_service.publish_scan_task(
                        contractor_id=contractor.id,
                        url=link,
                        depth=0,
                        session_id=scan_session.id if scan_session else None
                    )
                    added_to_queue += 1
                    await logger.debug(f"üì§ Added to queue: {link}")
            
            await logger.info(f"üì§ Added {added_to_queue} new pages to scan queue for contractor {contractor.id}")
            
        except Exception as e:
            await logger.error(f"‚ùå Error scanning page {url}: {e}")
            await logger.exception("Full traceback:")
    
    async def _fetch_page(self, url: str) -> Dict[str, Any] | None:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            start_time = datetime.utcnow()
            
            await logger.debug(f"üåê Making HTTP request to: {url}")
            async with self.session.get(url, allow_redirects=True) as response:
                response_time = (datetime.utcnow() - start_time).total_seconds()
                
                if response.status != 200:
                    await logger.warning(f"‚ö†Ô∏è HTTP {response.status} for {url}")
                    return None
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º Content-Type
                content_type = response.headers.get('content-type', '').lower()
                
                # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ HTML/—Ç–µ–∫—Å—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                if not any(ct in content_type for ct in ['text/html', 'text/plain', 'application/xhtml+xml']):
                    await logger.info(f"‚è≠Ô∏è Skipping non-HTML content: {content_type} for {url}")
                    return None
                
                try:
                    content = await response.text()
                except UnicodeDecodeError as e:
                    await logger.warning(f"‚ö†Ô∏è Unicode decode error for {url}: {e}")
                    return None
                
                content_length = len(content)
                
                await logger.debug(f"üì• Received {content_length} bytes from {url}")
                
                # –ü–∞—Ä—Å–∏–º HTML
                soup = BeautifulSoup(content, 'html.parser')
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
                text_content = soup.get_text(separator=' ', strip=True)
                text_length = len(text_content)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                title = soup.find('title')
                title_text = title.get_text().strip() if title else None
                
                meta_desc = soup.find('meta', attrs={'name': 'description'})
                description = meta_desc.get('content') if meta_desc else None
                
                await logger.debug(f"üìù Extracted {text_length} characters of text from {url}")
                
                return {
                    'html': content,
                    'text': text_content,
                    'title': title_text,
                    'description': description,
                    'http_status': response.status,
                    'response_time': response_time,
                    'url': url
                }
                
        except Exception as e:
            await logger.error(f"‚ùå Error fetching {url}: {e}")
            return None
    
    async def _save_webpage(self, contractor: Contractor, url: str, page_data: Dict[str, Any], scan_session: Optional['ScanSession'] = None) -> WebPage:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        from app.models.webpage import WebPage
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –≤ —Ä–∞–º–∫–∞—Ö —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏
        if scan_session:
            webpage = await WebPage.filter(
                contractor=contractor,
                url=url,
                scan_session=scan_session
            ).first()
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç —Å–µ—Å—Å–∏–∏, –∏—â–µ–º –ø–æ –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç—É –∏ URL
            webpage = await WebPage.filter(
                contractor=contractor,
                url=url
            ).first()
        
        created = webpage is None
        
        if created:
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            webpage = await WebPage.create(
                contractor=contractor,
                url=url,
                title=page_data.get('title'),
                meta_description=page_data.get('description'),
                content=page_data['html'],
                text_content=page_data['text'],
                status='completed',
                http_status=page_data.get('http_status'),
                response_time=page_data.get('response_time'),
                last_scanned=datetime.utcnow(),
                scan_session=scan_session
            )
            await logger.info(f"üìù Created new page: {url} in session {scan_session.id if scan_session else 'None'}")
        else:
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            await logger.info(f"üîÑ Updating existing page: {url}")
            webpage.title = page_data.get('title')
            webpage.meta_description = page_data.get('description')
            webpage.content = page_data['html']
            webpage.text_content = page_data['text']
            webpage.status = 'completed'
            webpage.http_status = page_data.get('http_status')
            webpage.response_time = page_data.get('response_time')
            webpage.last_scanned = datetime.utcnow()
            
            # –ï—Å–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –±—ã–ª–∞ –ø—Ä–∏–≤—è–∑–∞–Ω–∞ –∫ —Å–µ—Å—Å–∏–∏, –ø—Ä–∏–≤—è–∑—ã–≤–∞–µ–º
            if scan_session and not webpage.scan_session:
                webpage.scan_session = scan_session
                await logger.info(f"üîó Linked page {url} to session {scan_session.id}")
            
            await webpage.save()
        
        return webpage
    
    async def _check_violations(self, page_data: Dict[str, Any], forbidden_words: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞—Ä—É—à–µ–Ω–∏—è"""
        violations = []
        text = page_data['text']
        text_lower = text.lower()
        
        await logger.info(f"üîç Checking violations on page {page_data.get('url', 'unknown')}")
        await logger.info(f"üìù Text length: {len(text)} characters")
        await logger.info(f"üìã Found {len(forbidden_words)} forbidden words to check")
        
        for word_data in forbidden_words:
            await logger.info(f"üîç Checking: '{word_data['word']}' (regex: {word_data.get('use_regex', '?')}, case_sensitive: {word_data.get('case_sensitive', '?')})")
            word = word_data['word']
            use_regex = word_data.get('use_regex', False)
            case_sensitive = word_data.get('case_sensitive', False)
            
            search_text = text if case_sensitive else text_lower
            search_word = word if case_sensitive else word.lower()
            
            await logger.info(f"üîç Checking: '{word}' (regex: {use_regex}, case_sensitive: {case_sensitive})")
            
            if use_regex:
                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ
                    # –ï—Å–ª–∏ case_sensitive=True, –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º re.IGNORECASE
                    flags = 0 if case_sensitive else re.IGNORECASE
                    pattern = re.compile(search_word, flags)
                    matches = pattern.finditer(search_text)
                    
                    match_count = 0
                    for match in matches:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                        start = max(0, match.start() - 50)
                        end = min(len(search_text), match.end() + 50)
                        context = search_text[start:end]
                        
                        violations.append({
                            'word': word,
                            'position': match.start(),
                            'context': context,
                            'url': page_data.get('url', ''),
                            'matched_text': match.group()  # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
                        })
                        
                        match_count += 1
                        await logger.info(f"  ‚úÖ Found regex violation: '{match.group()}' for pattern '{word}'")
                    
                    if match_count == 0:
                        await logger.info(f"  ‚ùå No regex matches found for pattern '{word}'")
                        
                except re.error as e:
                    await logger.warning(f"  ‚ùå Error in regex '{word}': {e}")
                    continue
            else:
                # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ–¥—Å—Ç—Ä–æ–∫–∏
                if search_word in search_text:
                    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –≤—Ö–æ–∂–¥–µ–Ω–∏—è
                    matches = re.finditer(re.escape(search_word), search_text)
                    match_count = 0
                    for match in matches:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                        start = max(0, match.start() - 50)
                        end = min(len(search_text), match.end() + 50)
                        context = search_text[start:end]
                        
                        violations.append({
                            'word': word,
                            'position': match.start(),
                            'context': context,
                            'url': page_data.get('url', ''),
                            'matched_text': match.group()
                        })
                        
                        match_count += 1
                        await logger.info(f"  ‚úÖ Found word violation: '{match.group()}' for word '{word}'")
                    
                    if match_count == 0:
                        await logger.info(f"  ‚ùå No word matches found for '{word}'")
                else:
                    await logger.info(f"  ‚ùå Word '{word}' not found in text")
        
        await logger.info(f"üéØ Found {len(violations)} total violations on page {page_data.get('url', 'unknown')}")
        return violations
    
    async def _recalculate_contractor_stats(self, contractor: Contractor):
        """–ü–µ—Ä–µ—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç–∞"""
        from app.models.scan_result import Violation
        from app.models.scan_session import ScanSession
        
        # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞—Ä—É—à–µ–Ω–∏–π
        total_violations = await Violation.filter(webpage__contractor=contractor).count()
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–π (—Å–µ—Å—Å–∏–π)
        scan_sessions_count = await ScanSession.filter(contractor=contractor).count()
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞—Ä—É—à–µ–Ω–∏–π –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å–µ—Å—Å–∏–∏
        last_session = await ScanSession.filter(contractor=contractor).order_by('-started_at').first()
        last_session_violations = 0
        if last_session:
            last_session_violations = await Violation.filter(webpage__scan_session=last_session).count()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç–∞
        contractor.total_violations = total_violations
        contractor.scan_sessions_count = scan_sessions_count
        contractor.last_session_violations = last_session_violations
        await contractor.save()
        
        await logger.info(f"üìä Updated contractor stats: {contractor.name} - Total violations: {total_violations}, Sessions: {scan_sessions_count}, Last session violations: {last_session_violations}")

    async def _save_violations(self, webpage: WebPage, violations: List[Dict[str, Any]]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–∞—Ä—É—à–µ–Ω–∏–π"""
        from app.models.scan_result import Violation
        from app.models.forbidden_word import ForbiddenWord
        
        if not violations:
            return
        
        webpage.violations_found = True
        webpage.violations_count = len(violations)
        await webpage.save()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥–æ–µ –Ω–∞—Ä—É—à–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        for violation_data in violations:
            # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω–æ–µ —Å–ª–æ–≤–æ
            forbidden_word = await ForbiddenWord.get_or_none(word=violation_data['word'])
            if forbidden_word:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–æ–µ –Ω–∞—Ä—É—à–µ–Ω–∏–µ –Ω–∞ —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                # (–ø–æ –ø–æ–∑–∏—Ü–∏–∏ –∏ –Ω–∞–π–¥–µ–Ω–Ω–æ–º—É —Å–ª–æ–≤—É)
                existing_violation = await Violation.filter(
                    webpage=webpage,
                    forbidden_word=forbidden_word,
                    position=violation_data['position'],
                    word_found=violation_data.get('matched_text', violation_data['word'])
                ).first()
                
                if not existing_violation:
                    await Violation.create(
                        webpage=webpage,
                        forbidden_word=forbidden_word,
                        word_found=violation_data.get('matched_text', violation_data['word']),
                        context=violation_data['context'],
                        position=violation_data['position'],
                        severity=forbidden_word.severity
                    )
                    await logger.debug(f"üíæ Created new violation: '{violation_data.get('matched_text', violation_data['word'])}' at position {violation_data['position']}")
                else:
                    await logger.debug(f"‚è≠Ô∏è Skipped duplicate violation: '{violation_data.get('matched_text', violation_data['word'])}' at position {violation_data['position']}")
        
        # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç–∞
        contractor = webpage.contractor
        await self._recalculate_contractor_stats(contractor)
        
        await logger.info(f"üíæ Saved violations for page {webpage.url}")
    
    async def _extract_links(self, html: str, domain: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –∏–∑ HTML"""
        soup = BeautifulSoup(html, 'html.parser')
        links = []
        
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –∏ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏
            if href.startswith('/'):
                full_url = f"https://{domain}{href}"
            elif href.startswith('http'):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Å—ã–ª–∫–∞ –≤–µ–¥–µ—Ç –Ω–∞ —Ç–æ—Ç –∂–µ –¥–æ–º–µ–Ω
                parsed = urlparse(href)
                if parsed.netloc == domain:
                    full_url = href
                else:
                    continue
            else:
                continue
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏
            if any(skip in full_url.lower() for skip in ['#', 'javascript:', 'mailto:', 'tel:']):
                continue
            
            links.append(full_url)
        
        return list(set(links))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
scanner_service = ScannerService() 